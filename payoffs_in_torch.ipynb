{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext jupyter_black\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.stats.qmc import Halton\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 42\n",
    "g_cuda = torch.Generator(device=\"cuda\")\n",
    "g_cuda.manual_seed(42)\n",
    "sampler = Halton(d=6, scramble=True)\n",
    "\n",
    "parameter_space = dict(\n",
    "    spot=(30, 70),\n",
    "    path_integral=(25, 150),\n",
    "    ttm=(0.2, 1),\n",
    "    t=(0, 0.8),\n",
    "    vol=(0.1, 0.5),\n",
    "    r=(0, 0.1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The black scholes model for the spot price under risk neutral measure is:\n",
    "$$dS_t = rS_t\\,dt + \\sigma S_t\\,dW(t)$$\n",
    "Which is equivalent to:\n",
    "$$d\\log(S_t) = \\left(r - \\frac{\\sigma^2}{2}\\right)\\,dt + \\sigma\\,dW(t)$$\n",
    "The corresponding Euler-Maruyama scheme for a discrete grid $\\{t_k = \\frac{kT}{n},\\; k\\in\\{0,...,n\\}\\}$ is:\n",
    "$$d\\log(S_{t_{k+1}})  = d\\log(S_{t_k}) + \\left(r - \\frac{\\sigma^2}{2}\\right)\\frac{T}{n} + \\sigma \\sqrt\\frac{T}{n} Z_{k+1}$$\n",
    "Where $Z$ are i.i.d standard normal variables.\n",
    "\n",
    "We want to compute the following price :\n",
    "$$F(t, T, S_t, I_t, r, \\sigma) = e^{-r(T-t)}\\mathbb E\\left[\\left(S_T - I_T\\right)^+\\Big|\\,S_t, \\,I_t\\right]$$\n",
    "$$\\text{Where :}\\quad I_t = \\frac{1}{t}\\int_{0}^{t} S_u\\,du$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asian_option(\n",
    "    n_paths: int,\n",
    "    n_steps: int,\n",
    "    spot: float,\n",
    "    path_integral: float,\n",
    "    t: float,\n",
    "    dt: float,\n",
    "    r: float,\n",
    "    sigma: float,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Simulate n_paths of asian option payoffs\n",
    "    Args:\n",
    "        - n_paths: number of simulations\n",
    "        - n_steps: number of steps in the euler scheme\n",
    "        - spot: initial value of the asset\n",
    "        - path_integral: mean of the value of the asset at time t\n",
    "        - t: time of pricing\n",
    "        - dt: square root of time step\n",
    "        - r: interest rate\n",
    "        - sigma: volatility\n",
    "    Returns:\n",
    "        - torch.Tensor: sample payoffs\n",
    "    \"\"\"\n",
    "    T = t + dt * dt * n_steps\n",
    "    sample = torch.normal(\n",
    "        mean=torch.zeros(n_paths, n_steps).to(device), std=1, generator=g_cuda\n",
    "    )\n",
    "    sample = (r - 0.5 * sigma**2) * dt * dt + dt * sigma * sample\n",
    "    sample = torch.concatenate(\n",
    "        (\n",
    "            spot * torch.ones((n_paths, 1)).to(device),\n",
    "            spot * torch.exp(sample.cumsum(axis=1)),\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    sample = torch.exp(-r * dt * dt * n_steps) * torch.maximum(\n",
    "        sample[:, -1]\n",
    "        - t * path_integral / T\n",
    "        - dt * dt * n_steps * sample.mean(axis=1) / T,\n",
    "        torch.zeros(1).to(device),\n",
    "    )\n",
    "    # We use the same name variable sample to free up gpu memory of unecessary data\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NTPB = 1024\n",
    "NB = 1024\n",
    "# n = NB * NTPB\n",
    "n = 1000\n",
    "T = 1.0\n",
    "S_0 = 50.0\n",
    "sigma = 0.2\n",
    "r = 0.1\n",
    "N = 100\n",
    "t = 0.2\n",
    "I = 60.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.1886, device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "asian_option(int(1e6), N, S_0, I, t, torch.sqrt(torch.tensor(T / N)), r, sigma).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizing the payoffs sampler for batch operations\n",
    "vmap_asian_option = torch.vmap(\n",
    "    asian_option,\n",
    "    in_dims=(None, None, 0, 0, 0, 0, 0, 0),\n",
    "    out_dims=0,\n",
    "    randomness=\"different\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to learn $F(t, T, S_t, I_t, r, \\sigma)$ for a predefined grid of parameters via an MLP, this translates to finding:\n",
    "$$\\theta^* \\in \\argmin_{\\theta\\in\\Theta}\\mathbb E_{x\\sim D}\\left[(F(x) - T_\\theta(x))^2\\right]$$\n",
    "Where $x = (t, T, S_t, I_t, r, \\sigma)$ and $D$ is a prior distribution over the parameter space. Since we are attempting to learn an expectation, we can rewrite the problem as:\n",
    "$$\\theta^* \\in \\argmin_{\\theta\\in\\Theta}\\mathbb E_{x\\sim D}\\left[\\mathbb E_{(S_T, I_T)}\\left[ (S_T - I_T)^+ - T_\\theta(x))^2 \\Big | x\\right]\\right]$$\n",
    "\n",
    "We can thus train our network on payoffs and test it agaisnt MC estimations of the price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling parameters from a grid\n",
    "n = 1000\n",
    "N = 100\n",
    "nb_samples = int(1e6)\n",
    "sample_params = sampler.random(n=nb_samples)\n",
    "sample_params = np.array(\n",
    "    [\n",
    "        parameter_space[\"spot\"][1] - parameter_space[\"spot\"][0],\n",
    "        parameter_space[\"path_integral\"][1] - parameter_space[\"path_integral\"][0],\n",
    "        parameter_space[\"t\"][1] - parameter_space[\"t\"][0],\n",
    "        parameter_space[\"ttm\"][1] - parameter_space[\"ttm\"][0],\n",
    "        parameter_space[\"r\"][1] - parameter_space[\"r\"][0],\n",
    "        parameter_space[\"vol\"][1] - parameter_space[\"vol\"][0],\n",
    "    ]\n",
    ") * sample_params + np.array(\n",
    "    [\n",
    "        parameter_space[\"spot\"][0],\n",
    "        parameter_space[\"path_integral\"][0],\n",
    "        parameter_space[\"t\"][0],\n",
    "        parameter_space[\"ttm\"][0],\n",
    "        parameter_space[\"r\"][0],\n",
    "        parameter_space[\"vol\"][0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "sample_params[:, 1] = (sample_params[:, 3] < 0.05) * sample_params[:, 0] + (\n",
    "    sample_params[:, 3] >= 0.05\n",
    ") * sample_params[:, 0] * np.random.uniform(low=0.5, high=2, size=len(sample_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate parameters into batches\n",
    "sample_params = torch.tensor(sample_params).to(device)\n",
    "data_loader = torch.utils.data.DataLoader(sample_params, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute n payoffs per sample param\n",
    "results = deque()\n",
    "for sample in data_loader:\n",
    "    sample_payoffs = vmap_asian_option(\n",
    "        n,\n",
    "        N,\n",
    "        sample[:, 0],\n",
    "        sample[:, 1],\n",
    "        sample[:, 2],\n",
    "        torch.sqrt(sample[:, 3] / N),\n",
    "        sample[:, 4],\n",
    "        sample[:, 5],\n",
    "    ).mean(axis=1)\n",
    "    sample_payoffs = sample_payoffs.to(\"cpu\")\n",
    "    results.append(sample_payoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving data\n",
    "X_train = sample_params.to(\"cpu\").numpy()\n",
    "with open(\"X_train.npy\", \"wb\") as f:\n",
    "    np.save(f, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Y_train_averaged.npy\", \"wb\") as f:\n",
    "    np.save(f, torch.cat(list(results), dim=0).numpy(), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = NB * NTPB\n",
    "N = 100\n",
    "nb_samples = int(1e4)\n",
    "sample_params = sampler.random(n=nb_samples)\n",
    "sample_params = np.array(\n",
    "    [\n",
    "        parameter_space[\"spot\"][1] - parameter_space[\"spot\"][0],\n",
    "        parameter_space[\"path_integral\"][1] - parameter_space[\"path_integral\"][0],\n",
    "        parameter_space[\"t\"][1] - parameter_space[\"t\"][0],\n",
    "        parameter_space[\"ttm\"][1] - parameter_space[\"ttm\"][0],\n",
    "        parameter_space[\"r\"][1] - parameter_space[\"r\"][0],\n",
    "        parameter_space[\"vol\"][1] - parameter_space[\"vol\"][0],\n",
    "    ]\n",
    ") * sample_params + np.array(\n",
    "    [\n",
    "        parameter_space[\"spot\"][0],\n",
    "        parameter_space[\"path_integral\"][0],\n",
    "        parameter_space[\"t\"][0],\n",
    "        parameter_space[\"ttm\"][0],\n",
    "        parameter_space[\"r\"][0],\n",
    "        parameter_space[\"vol\"][0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "sample_params[:, 1] = (sample_params[:, 3] < 0.05) * sample_params[:, 0] + (\n",
    "    sample_params[:, 3] >= 0.05\n",
    ") * sample_params[:, 0] * np.random.uniform(low=0.5, high=2, size=len(sample_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate parameters into batches\n",
    "sample_params = torch.tensor(sample_params).to(device)\n",
    "data_loader = torch.utils.data.DataLoader(sample_params, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute n payoffs per sample param and retrieve mc estimator of price\n",
    "results = deque()\n",
    "for sample in data_loader:\n",
    "    sample_payoffs = vmap_asian_option(\n",
    "        n,\n",
    "        N,\n",
    "        sample[:, 0],\n",
    "        sample[:, 1],\n",
    "        sample[:, 2],\n",
    "        torch.sqrt(sample[:, 3] / N),\n",
    "        sample[:, 4],\n",
    "        sample[:, 5],\n",
    "    ).mean(axis=1)\n",
    "    sample_payoffs = sample_payoffs.to(\"cpu\")\n",
    "    results.append(sample_payoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving data\n",
    "X_valid = sample_params.to(\"cpu\").numpy()\n",
    "with open(\"X_valid.npy\", \"wb\") as f:\n",
    "    np.save(f, X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Y_valid.npy\", \"wb\") as f:\n",
    "    np.save(f, torch.cat(list(results), dim=0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
